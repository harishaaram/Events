{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory book analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An important step when working with the NLTK package is making sure that all the modules, algorithms, corpora, lists are available when running the program/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/harish/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/harish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'urllib2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8c2a8f12413d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;31m# imports string module that allows running processs on strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;31m# imports punctuation symbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0murllib2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m \u001b[0;31m# imports module for fetching and processing internet resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m \u001b[0;31m#this module allows interfacing with the underlying operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m \u001b[0;31m#imports regular expression module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'urllib2'"
     ]
    }
   ],
   "source": [
    "#The following commands import the modules, algorithms, corpora, lists that will be needed for exploratory book analysis\n",
    "import nltk #imports the nltk module\n",
    "nltk.download(\"words\") # downloads English words\n",
    "nltk.download(\"stopwords\") # downloads English stopwords \n",
    "from nltk.text import Text #imports the Text module that enables the exploratory search of the text\n",
    "from nltk import bigrams # imports the bigrams module\n",
    "from nltk.collocations import * # imports the collocations methods\n",
    "import string # imports string module that allows running processs on strings\n",
    "from string import punctuation # imports punctuation symbols\n",
    "from urllib2 # imports module for fetching and processing internet resources\n",
    "import os #this module allows interfacing with the underlying operating system\n",
    "import re #imports regular expression module \n",
    "from collections import OrderedDict #imports collections module, an alternative to list, dict, set, tuple\n",
    "from nltk.draw import dispersion_plot\n",
    "from IPython.display import display # a module that wraps the object to be displayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening up and reading the text of a book available on the Project Gutenberg web site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/140/140-0.txt\" #assigning a link to a variable\n",
    "data = urllib2.urlopen(url.encode('utf-8')).read() #opening up the link/file\n",
    "data = data.replace(\"\\r\\n\", \" \")\n",
    "data = unicode(data, errors=\"ignore\") #ignores encoding errors\n",
    "data[:10000]\n",
    "#Alternatives:\n",
    "# https://www.gutenberg.org/files/140/140-0.txt The Jungle by Upton Sinclair\n",
    "#https://www.gutenberg.org/files/543/543-0.txt # Main Street by Sinclair Lewis\n",
    "#https://www.gutenberg.org/files/1342/1342-0.txt Pride and Prejudice by Jane Austin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional processing of the file, removing text metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = re.sub('^(.*?)START OF THIS PROJECT GUTENBERG EBOOK', '', data) # remove anything from the start of the string to the first occurence of Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data [:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(data) # how long the entire string/file is (how many characters)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[750000:] # checking the end of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the Project Gutenberg declaration at the end of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = re.sub('End of the Project Gutenberg(.*?)$', '', data) #removing the Project Gutenberg declaration at the end of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the information that we don't consider to be the content of the book has been removed, we proceed with tokenizing the book into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_string = nltk.word_tokenize(data) # the entire text file is tokenized into individual words by using the nltk word_tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(words_string) #checking the lenght of the word list created, how many words in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_string[200:500] #checking the word list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords from the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english')) # defines stopwords list to use (part of NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extending the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morewords =['--'] # adding more stopwords to the list of stopwords (this list can be further customized)\n",
    "stopwords.update(morewords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing words that are less than a character long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_string = [word for word in words_string if len(word) > 1] # removes words that are less than a character long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_string = [word.lower() for word in words_string] # lowercases all the words in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_string = [word for word in words_string if word not in stopwords] # removes stopwords from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(words_string) # notice a sharp decrease of the number of words after stopwords have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book explorations: concordances and collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textList = Text(words_string) # creates a Text object based on the list of words from the text; this text object allows exploratory analysis of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(textList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textList[6050:8500] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textList.concordance('chicago') # the text object allows us to examine concordances of different words in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textList.similar('chicago', 20) # what are the words that appear in similar contexts to the word 'jurgis'? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textIndex = nltk.text.ContextIndex(textList) # builds a textIndex object based on the textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textIndex.word_similarity_dict('chicago')['packingtown'] #how similar are 'carriage' and 'elizabeth'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textIndex.common_contexts(['chicago', 'packingtown']) # what are the common contexts for the words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textList.dispersion_plot(['chicago', 'packingtown']) #outputs a dispersion plot for words 'chicago' and 'packingtown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establishing collocations in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textList.collocations() # obtains collocations (\"the habitual juxtaposition of a particular word with another word or words with a frequency greater than chance\"; \"a conventional way of saying things\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring bigrams (a pair of consecutive written units, in our case words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures() # obtains bigram association measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(words_string, 5) # finds bigrams in the text\n",
    "finder.apply_freq_filter(5) #removes bigrams that appear less than 5 times\n",
    "bigrams = finder.nbest(bigram_measures.likelihood_ratio, 1000) #obtain the bigrams with the highest likelihood ratio\n",
    "print '\\n'.join('%s %s' % v for v in bigrams) #printing them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine frequency distribution of the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(words_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word, frequency in fdist.most_common(100): #finds most common words in the text\n",
    "    display (\"%s %d\" % (str(word), frequency)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative distribution of 50 most frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the window size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we are interested in examining the surrounding context of a particular word in the text, we need to decide on the parameters of that context.\n",
    "# If, for example, we are interested in three words that precede a particular word and three words after then we are interested in a 7-word window size.\n",
    "# This window size can be established as follows:\n",
    "ngrams = [words_string[i:i+7] for i in range(len(words_string)-6)]# http://digitalhistoryhacks.blogspot.com/2006/08/easy-pieces-in-python-keyword-in.html\n",
    "    \n",
    "kwicdict = {}           # opens up a kwicdict        \n",
    "for n in ngrams: #indexes the fourth word in a sliding window of 7 (three words on each side) \n",
    "    if n[3] not in kwicdict: \n",
    "        kwicdict[n[3]] = [n]\n",
    "    else:\n",
    "        kwicdict[n[3]].append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyword in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in kwicdict['chicago']: # nicely formatted output for the word we are interested in determining the context for\n",
    "    outstring = ' '.join(n[:3]).rjust(30) #output as a string and right justify first three words in the sliding window\n",
    "    outstring += str(n[3]).center(len(n[3])+6) # output as a string and center fourth word in the sliding window\n",
    "    outstring += ' '.join(n[4:]) # output as a string last three words in the sliding window of 7 consecutive words in the text\n",
    "    print outstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
