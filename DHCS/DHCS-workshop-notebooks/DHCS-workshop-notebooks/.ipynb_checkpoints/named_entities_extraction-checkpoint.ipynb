{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entities extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After the exploratory analysis, the focus is on identifying and extracting named entities in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk # imports nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/harish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/harish/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/harish/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/harish/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/harish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\") # downloads punkt tokenizer models\n",
    "nltk.download('averaged_perceptron_tagger') # downloads the algorithm for predicting the part of speech information\n",
    "nltk.download('maxent_ne_chunker') # downloads the maximum entropy chunker that has been trained on the ACE 2004 corpus https://catalog.ldc.upenn.edu/LDC2005T09\n",
    "nltk.download('words') #downloads the list of English language words\n",
    "nltk.download('stopwords') # downloads the list of stopwords\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk #imports a word tokenizer, part of speech tagger and named entity tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import string\n",
    "import urllib2\n",
    "import os\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import codecs\n",
    "codecs.register_error(\"strict\", codecs.ignore_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Function for extracting named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entity_names(t, entity_type): # this function has been defined here: https://gist.github.com/onyxfish/322906\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == entity_type:\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child, entity_type))\n",
    "\n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for converting a list to a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_dict(li):  \n",
    "     dct = {}  \n",
    "     for item in li:  \n",
    "         if dct.has_key(item):  \n",
    "             dct[item] = dct[item] + 1  \n",
    "         else:  \n",
    "             dct[item] = 1  \n",
    "     return dct  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### File opening and reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/140/140-0.txt\" #assigns a link to a variable\n",
    "data = urllib2.urlopen(url.encode('utf-8')).read() #opens up the link/file\n",
    "data = data.replace(\"\\r\\n\", \" \")\n",
    "data = re.sub('^(.*?)START OF THIS PROJECT GUTENBERG EBOOK', '', data) #removes Project Gutenberg metadata\n",
    "data = re.sub('End of the Project Gutenberg(.*?)$', '', data)#removes Project Gutenberg metadata\n",
    "data = unicode(data, errors=\"ignore\") #ignores encoding errors\n",
    "data[:10000]\n",
    "#Alternative texts to use\n",
    "#https://www.gutenberg.org/files/140/140-0.txt The Jungle by Upton Sinclair\n",
    "# https://www.gutenberg.org/files/1342/1342-0.txt Pride and Prejudice by Jane Austen\n",
    "#https://www.gutenberg.org/files/543/543-0.txt The Main Street by Sinclair Lewis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(data) #tokenizes the file into sentences\n",
    "sentences [20:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senten = len(sentences)\n",
    "senten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [nltk.word_tokenize(sent) for sent in sentences] # rather than tokenizing the entire file that has been converted into a string, this function tokenizes each of the sentence -- the sentence boundaries are maintained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words[50:70] # notice the sentence boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting part of speech information from the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences = [nltk.pos_tag(word) for word in words] # for each sentence in words (sentence boundaries are maintained in words) assign the most probable part of speech for each word in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%store tagged_sentences >> pos_pride_and_prejudice.txt\n",
    "#%store tagged_sentences >> pos_the_main_street.txt\n",
    "#%store tagged_sentences >> pos_the_jungle.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences[100:150] # notice the sentence boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#locations = [] # opens an empty list for locations\n",
    "gpe =[] #opens an empty list for geopolitical entities\n",
    "#persons = [] #opens an empty list for persons\n",
    "#organizations = [] #opens an empty list for organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting named entities from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in tagged_sentences: # for each part of speech tagged sentence in a tagged_sentences list, extract named entities based on the chunking\n",
    "    chunked_sentence = nltk.ne_chunk(i, binary=False) # predict which noun phrases in the text are named entities. If binary is set to True, ask the model to predict whether something is a named entity or not. If set to False, the model predicts the type of named entity.   \n",
    "    gpe.extend(extract_entity_names(chunked_sentence, 'GPE'))#extract geopolitical entities from the text\n",
    "    #locations.extend(extract_entity_names(chunked_sentence, 'LOCATION')) # extract loctions from the text\n",
    "    #persons.extend(extract_entity_names(chunked_sentence, 'PERSON')) #extract persons from the text\n",
    "    #organizations.extend(extract_entity_names(chunked_sentence, 'ORGANIZATION')) #extract organizations from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = len(gpe)\n",
    "#length = len(locations)\n",
    "#length = len(persons)\n",
    "#length = len(organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(gpe) #get a unique set of geopolitical entities from the text\n",
    "#set(locations)\n",
    "#set(persons)\n",
    "#set(organizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#di_locations = list_to_dict(locations) # convert a list to a dictionary that counts the occurrences of each of geographical entity\n",
    "#di_persons = list_to_dict(persons)\n",
    "#di_organizations = list_to_dict(organizations)\n",
    "di_gpe = list_to_dict(gpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#di_locations = OrderedDict(sorted(di_locations.items(), key=lambda t: t[1], reverse=True))\n",
    "#di_persons = OrderedDict(sorted(di_persons.items(), key=lambda t: t[1], reverse=True))\n",
    "#di_organizatios = OrderedDict(sorted(di_organizations.items(), key=lambda t: t[1], reverse=True))\n",
    "di_gpe = OrderedDict(sorted(di_gpe.items(), key=lambda t: t[1], reverse=True)) #sort the dictionary in a decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#di_locations\n",
    "#di_persons\n",
    "#di_organizations\n",
    "di_gpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%store di_persons >> dic_pride_and_prejudice_per.txt \n",
    "#%store di_locations >> dic_pride_and_prejudice_loc.txt\n",
    "#%store di_organizations >> dic_pride_and_prejudice_org.txt\n",
    "\n",
    "#%store di_persons >> dic_main_street_per.txt\n",
    "#%store di_locations >> dic_main_street_per_loc.txt\n",
    "#%store di_organizations >> dic_main_street_org_org.txt\n",
    "\n",
    "#%store di_persons >> dic_the_jungle_per.txt\n",
    "#%store di_locations >> dic_the_jungle_loc.txt\n",
    "#%store di_organizations >> dic_the_jungle_org.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nounphrases = []\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" #running regular expressions on part of speech to obtain noun phrase constituents from the text\n",
    "for sent in tagged_sentences:\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(sent)\n",
    "    for sent in result.subtrees():\n",
    "            if sent.label() == \"NP\":\n",
    "                nounphrases.append(str(sent.leaves()))\n",
    "for i in nounphrases:\n",
    "    print (i)\n",
    "\n",
    "#print (nounphrases)\n",
    "#%store nounphrases >> nounphrases_pride_and_prejudice.txt\n",
    "#%store nounphrases >> nounphrases_main_street.txt\n",
    "#%store nounphrases >> nounphrases_the_jungle.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
